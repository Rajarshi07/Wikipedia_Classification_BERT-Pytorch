{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: \n",
    "1. Create Folders based on unique rating values and create empty files inside folders with filename=article_pageid\n",
    "2. Download articles from wikipedia and save as file. Use article_pageid as page id to search in wikipedia api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUNNING IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "import wikipedia\n",
    "import queue\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDIT THIS TAB TO CONFIGURE NOTEBOOK SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "BASE_FOLDER = \"TEST\"\n",
    "TSV_FILE = \"test_set.tsv\"\n",
    "# Group into folder by this column\n",
    "FOLDER_BY = \"rating\"\n",
    "# Create files with names from this column\n",
    "FILE_BY = \"article_pageid\"\n",
    "FILE_FORMAT = \".txt\"\n",
    "# THREADS\n",
    "THREAD_COUNT = 40\n",
    "# ERROR FILE\n",
    "ERROR_FILE = \"ERROR_LIST.pickle\"\n",
    "# SAFE CHARACTERS IN FILENAMES\n",
    "SAFE_CHARACTERS = (\".\", \"_\", \"-\")\n",
    "# CLEANING CONFIG\n",
    "DELETE_SECTIONS = [\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"See also\",\n",
    "    \"Further reading\",\n",
    "    \"Notes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "]\n",
    "CLEANING_LEVEL = 0\n",
    "\"\"\"\n",
    "    WARNING:\n",
    "        Don't clean data while downloading if you plan on using different levels of cleaning later on.\n",
    "        Cleaning headings will make it impossible use clean functions later on.\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED: \n",
    "        0: No Cleaning\n",
    "        3: Delete Sections only\n",
    "\"\"\"\n",
    "# END CONFIG\n",
    "\n",
    "## UTILITY FUNCTIONS TO MAKE CONFIG USABLE\n",
    "\n",
    "\n",
    "def _headingClean(x):\n",
    "    \"\"\"\n",
    "    internal function to clean headings and make them lowercase so that comparisons can be performed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return x.replace(\" \", \"\").lower()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR at clean_heading function:\", e)\n",
    "        return x\n",
    "\n",
    "\n",
    "DELETE_SECTIONS = list(map(_headingClean, DELETE_SECTIONS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UTILITY FUNCTIONS FOR CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_HEADINGS = re.compile(r\"==.*?==+\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def cleanHeadings(x, DEL_HEADINGS=False):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted characters from headings\n",
    "    Configurable to remove headings or not via DELETE_HEADINGS.\n",
    "    Configurable to clean headings or not via CLEAN_HEADINGS.\n",
    "    Warning: This function will remove all headings from the text. Please run only after deleting unwanted sections.\n",
    "    \"\"\"\n",
    "    if DEL_HEADINGS:\n",
    "        return RE_HEADINGS.sub(\"\", x)\n",
    "    else:\n",
    "        return (\n",
    "            x.replace(\"==== \", \"\")\n",
    "            .replace(\"=== \", \"\")\n",
    "            .replace(\"== \", \"\")\n",
    "            .replace(\" ====\", \"\")\n",
    "            .replace(\" ===\", \"\")\n",
    "            .replace(\" ==\", \"\")\n",
    "        )\n",
    "\n",
    "\n",
    "def removeSections(x):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted sections from the text\n",
    "    Configurable via DELETE_SECTIONS\n",
    "    \"\"\"\n",
    "    r = RE_HEADINGS.finditer(x)\n",
    "    sections = [(m.start(0), m.end(0)) for m in r]\n",
    "    s = []\n",
    "    for i, sec in enumerate(sections):\n",
    "        secname = x[sec[0] : sec[1]].replace(\"=\", \"\").replace(\" \", \"\").lower()\n",
    "        if secname in DELETE_SECTIONS:\n",
    "            sb = sec[0]\n",
    "            try:\n",
    "                se = sections[i + 1][0]\n",
    "            except IndexError:\n",
    "                se = len(x)\n",
    "            s.append(x[sb:se])\n",
    "    for sec in s:\n",
    "        x = x.replace(sec, \"\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    \"\"\"\n",
    "    Function to clean the text\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED:\n",
    "        0: No Cleaning\n",
    "        1: Light Cleaning\n",
    "        4: Heavy Cleaning\n",
    "    \"\"\"\n",
    "    if CLEANING_LEVEL == 0:\n",
    "        return x\n",
    "    elif CLEANING_LEVEL == 1:\n",
    "        return cleanHeadings(x)\n",
    "    elif CLEANING_LEVEL == 2:\n",
    "        return cleanHeadings(x, DEL_HEADINGS=True)\n",
    "    elif CLEANING_LEVEL == 3:\n",
    "        return removeSections(x)\n",
    "    elif CLEANING_LEVEL == 4:\n",
    "        return cleanHeadings(removeSections(x))\n",
    "    elif CLEANING_LEVEL == 5:\n",
    "        return cleanHeadings(removeSections(x), DEL_HEADINGS=True)\n",
    "    else:\n",
    "        raise Exception(\"Invalid CLEANING_LEVEL configured. Please check the config.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING THE TSV FILE AND CREATING INFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TSV_FILE, delimiter=\"\\t\")\n",
    "df.head()\n",
    "file_dict = dict(zip(df[FILE_BY], df[FOLDER_BY]))\n",
    "folder_dict = dict(zip(df[FOLDER_BY], df[FILE_BY]))\n",
    "urllist = df[FILE_BY].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder created: TEST\\FA\n",
      "folder created: TEST\\GA\n",
      "folder created: TEST\\B\n",
      "folder created: TEST\\C\n",
      "folder created: TEST\\Start\n",
      "folder created: TEST\\Stub\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(BASE_FOLDER):\n",
    "    os.mkdir(BASE_FOLDER)\n",
    "for folder in folder_dict.keys():\n",
    "    os.makedirs(os.path.join(BASE_FOLDER, folder), exist_ok=True)\n",
    "    print(f\"folder created: {os.path.join(BASE_FOLDER, folder)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN ASYNC WORKER CLASS AND RUNNER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_LIST = []\n",
    "\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, q, *args, **kwargs):\n",
    "        self.q = q\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                work = self.q.get(timeout=3)\n",
    "                print(\n",
    "                    f\"{self.name} working on {work} with {self.q.qsize()} items remaining\"\n",
    "                )\n",
    "                page = wikipedia.page(pageid=work)\n",
    "                title = \"\".join(\n",
    "                    c for c in page.title if c.isalnum() or c in SAFE_CHARACTERS\n",
    "                ).rstrip()\n",
    "                content = clean(page.content.encode(\"ascii\", \"ignore\").decode(\"ascii\"))\n",
    "                folder = file_dict[work]\n",
    "                with open(\n",
    "                    os.path.join(BASE_FOLDER, folder, f\"{title}{FILE_FORMAT}\"), \"w\"\n",
    "                ) as f:\n",
    "                    f.write(content)\n",
    "                    f.close()\n",
    "                    print(\n",
    "                        f\"{os.path.join(BASE_FOLDER, folder, f'{title}{FILE_FORMAT}')} written\"\n",
    "                    )\n",
    "            except queue.Empty:\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"{self.name} error\", e)\n",
    "                ERROR_LIST.append(work)\n",
    "            self.q.task_done()\n",
    "\n",
    "\n",
    "def RunWorkers(urllist):\n",
    "    q = queue.Queue()\n",
    "    global ERROR_LIST\n",
    "    ERROR_LIST = []\n",
    "    for work in urllist:\n",
    "        q.put_nowait(work)\n",
    "    for _ in range(THREAD_COUNT):\n",
    "        Worker(q).start()\n",
    "    q.join()\n",
    "    # INDEX DATASET INTO TSV_FILE\n",
    "    indexlist = []\n",
    "    for folder in folder_dict.keys():\n",
    "        for file in glob(os.path.join(BASE_FOLDER,folder, \"*.txt\")):\n",
    "            filename = os.path.basename(file)\n",
    "            indexlist.append([folder,filename])\n",
    "    table = pd.DataFrame(indexlist, columns=[\"category\", \"filename\"])\n",
    "    table.to_csv(os.path.join(BASE_FOLDER,\"index.tsv\"), sep=\"\\t\", index=True)\n",
    "    print(f\"{len(ERROR_LIST)} errors\")\n",
    "    pickle.dump(ERROR_LIST, open(ERROR_FILE, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THIS FOR FIRST RUN / DOWNLOAD FROM CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunWorkers(urllist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THIS FOR ERROR CORRECTION FROM PREVIOUS RUN\n",
    "Run this 2-3 times to remove all errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRORS = pickle.load(open(ERROR_FILE, \"rb\"))\n",
    "print(f\"Errors: {len(ERRORS)}\")\n",
    "\n",
    "RunWorkers(ERRORS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXING DATASET\n",
    "Run this once to index the dataset if already downloaded. This automatically happens if dataset is being downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX DATASET INTO TSV_FILE\n",
    "indexlist = []\n",
    "for folder in folder_dict.keys():\n",
    "    for file in glob(os.path.join(BASE_FOLDER,folder, \"*.txt\")):\n",
    "        filename = os.path.basename(file)\n",
    "        indexlist.append([folder,filename])\n",
    "table = pd.DataFrame(indexlist, columns=[\"category\", \"filename\"])\n",
    "table.to_csv(os.path.join(BASE_FOLDER,\"index.tsv\"), sep=\"\\t\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH DATASET CREATION\n",
    "from torch.utils.data import Dataset,random_split\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_file,\n",
    "        root_dir,\n",
    "        clean_level=0,\n",
    "        delete_sections=[\n",
    "            \"References\",\n",
    "            \"External links\",\n",
    "            \"See also\",\n",
    "            \"Further reading\",\n",
    "            \"Notes\",\n",
    "            \"Bibliography\",\n",
    "            \"Sources\",\n",
    "        ],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the tsv file with annotations.\n",
    "            root_dir (string): Database Base Directory.\n",
    "            clean_level (callable, optional): Optional cleaning to be applied on a sample.\n",
    "            delete_sections (list, optional): Optional list of sections to be deleted if clean level is configured to delete sections.\n",
    "        \"\"\"\n",
    "        self.wiki_frame = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "        self.root_dir = root_dir\n",
    "        self.clean_level = clean_level\n",
    "        self.RE_HEADINGS = re.compile(r\"==.*?==+\", re.MULTILINE)\n",
    "        self.delete_sections = list(map(self._headingClean, delete_sections))\n",
    "\n",
    "    def _headingClean(self,x,args=None):\n",
    "        \"\"\"\n",
    "            internal function to clean headings and make them lowercase so that comparisons can be performed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return x.replace(\" \", \"\").lower()\n",
    "        except Exception as e:\n",
    "            print(\"ERROR at clean_heading function:\", e)\n",
    "            return x\n",
    "\n",
    "    def cleanHeadings(self, x, DEL_HEADINGS=False):\n",
    "        \"\"\"\n",
    "            Function to remove unwanted characters from headings\n",
    "        \"\"\"\n",
    "        if DEL_HEADINGS:\n",
    "            return RE_HEADINGS.sub(\"\", x)\n",
    "        else:\n",
    "            return (\n",
    "                x.replace(\"==== \", \"\")\n",
    "                .replace(\"=== \", \"\")\n",
    "                .replace(\"== \", \"\")\n",
    "                .replace(\" ====\", \"\")\n",
    "                .replace(\" ===\", \"\")\n",
    "                .replace(\" ==\", \"\")\n",
    "            )\n",
    "\n",
    "    def removeSections(self, x):\n",
    "        \"\"\"\n",
    "            Function to remove unwanted sections from the text\n",
    "        \"\"\"\n",
    "        r = self.RE_HEADINGS.finditer(x)\n",
    "        sections = [(m.start(0), m.end(0)) for m in r]\n",
    "        s = []\n",
    "        for i, sec in enumerate(sections):\n",
    "            secname = x[sec[0] : sec[1]].replace(\"=\", \"\").replace(\" \", \"\").lower()\n",
    "            if secname in self.delete_sections:\n",
    "                sb = sec[0]\n",
    "                try:\n",
    "                    se = sections[i + 1][0]\n",
    "                except IndexError:\n",
    "                    se = len(x)\n",
    "                s.append(x[sb:se])\n",
    "        for sec in s:\n",
    "            x = x.replace(sec, \"\")\n",
    "        return x\n",
    "\n",
    "    def clean(self, x):\n",
    "        \"\"\"\n",
    "            Function to clean the text\n",
    "            CLEANING LEVELS:\n",
    "                0: No cleaning\n",
    "                1: Clean All Headings\n",
    "                2: Delete All Headings\n",
    "                3: Delete Sections only\n",
    "                4: Delete Selected Sections and Clean Headings\n",
    "                5: Delete Selected Sections and Delete All Headings\n",
    "            RECOMMENDED:\n",
    "                0: No Cleaning\n",
    "                1: Light Cleaning\n",
    "                4: Heavy Cleaning\n",
    "        \"\"\"\n",
    "        if self.clean_level == 0:\n",
    "            return x\n",
    "        elif self.clean_level == 1:\n",
    "            return cleanHeadings(x)\n",
    "        elif self.clean_level == 2:\n",
    "            return cleanHeadings(x, DEL_HEADINGS=True)\n",
    "        elif self.clean_level == 3:\n",
    "            return removeSections(x)\n",
    "        elif self.clean_level == 4:\n",
    "            return cleanHeadings(removeSections(x))\n",
    "        elif self.clean_level == 5:\n",
    "            return cleanHeadings(removeSections(x), DEL_HEADINGS=True)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid clean_level configured. Please reinitialize dataloader.\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wiki_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        x = self.wiki_frame.iloc[idx]\n",
    "        cat, file_name = x[\"category\"], x[\"filename\"]\n",
    "        txt = open(os.path.join(self.root_dir, cat, file_name), \"r\").read()\n",
    "        txt = self.clean(txt)\n",
    "        sample = {\"text\": txt, \"label\": cat}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = 'TEST/'\n",
    "INDEX_PATH = os.path.join(BASE_FOLDER,\"index.tsv\")\n",
    "CLEAN_LEVEL = 4\n",
    "CACHE_DIR = os.path.join(os.getcwd(), 'transformers-cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikipediaDataset(INDEX_PATH, BASE_FOLDER, clean_level=CLEAN_LEVEL)\n",
    "length = len(dataset)\n",
    "print(f\"Dataset length: {length}\")\n",
    "test_l = int(length * 0.15)\n",
    "train_l = length - test_l\n",
    "valid_l = int(train_l * 0.1)\n",
    "train_l -= valid_l\n",
    "train,test,validation = random_split(range(len(dataset)), [train_l,test_l,valid_l], generator=torch.Generator().manual_seed(42))\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}, Validation: {len(validation)}\")\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'B': 0, 'C': 1, 'FA': 2, 'GA': 3, 'Start': 4, 'Stub': 5}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6bdd94c198576e93dd5c2f539d34e9906db66a8c6bc385acd1638e2dac1003d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
