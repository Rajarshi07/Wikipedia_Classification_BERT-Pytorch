{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import RobertaTokenizerFast\n",
    "from packaging.version import Version\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, sys, os, datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Functions and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLEANING CONFIG\n",
    "DELETE_SECTIONS = [\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"See also\",\n",
    "    \"Further reading\",\n",
    "    \"Notes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "]\n",
    "CLEANING_LEVEL = 1\n",
    "\"\"\"\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED: \n",
    "        0: No Cleaning\n",
    "        1: Light Cleaning\n",
    "        4: Heavy Cleaning\n",
    "\"\"\"\n",
    "# END CONFIG\n",
    "\n",
    "## UTILITY FUNCTIONS TO MAKE CONFIG USABLE\n",
    "\n",
    "\n",
    "def _heading_clean(x):\n",
    "    \"\"\"\n",
    "    internal function to clean headings and make them lowercase so that comparisons can be performed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return x.replace(\" \", \"\").lower()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR at clean_heading function:\", e)\n",
    "        return x\n",
    "\n",
    "\n",
    "DELETE_SECTIONS = list(map(_heading_clean, DELETE_SECTIONS))\n",
    "\n",
    "## END UTILITY FUNCTIONS\n",
    "\n",
    "## CLEANER FUNCTIONS\n",
    "RE_HEADINGS = re.compile(r\"==.*?==+\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def clean_headings(x, DEL_HEADINGS=False):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted characters from headings\n",
    "    Configurable to remove headings or not via DELETE_HEADINGS.\n",
    "    Configurable to clean headings or not via CLEAN_HEADINGS.\n",
    "    Warning: This function will remove all headings from the text. Please run only after deleting unwanted sections.\n",
    "    \"\"\"\n",
    "    if DEL_HEADINGS:\n",
    "        return RE_HEADINGS.sub(\"\", x)\n",
    "    else:\n",
    "        return (\n",
    "            x.replace(\"==== \", \"\")\n",
    "            .replace(\"=== \", \"\")\n",
    "            .replace(\"== \", \"\")\n",
    "            .replace(\" ====\", \"\")\n",
    "            .replace(\" ===\", \"\")\n",
    "            .replace(\" ==\", \"\")\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_sections(x):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted sections from the text\n",
    "    Configurable via DELETE_SECTIONS\n",
    "    \"\"\"\n",
    "    r = RE_HEADINGS.finditer(x)\n",
    "    sections = [(m.start(0), m.end(0)) for m in r]\n",
    "    s = []\n",
    "    for i, sec in enumerate(sections):\n",
    "        secname = x[sec[0] : sec[1]].replace(\"=\", \"\").replace(\" \", \"\").lower()\n",
    "        if secname in DELETE_SECTIONS:\n",
    "            sb = sec[0]\n",
    "            try:\n",
    "                se = sections[i + 1][0]\n",
    "            except IndexError:\n",
    "                se = len(x)\n",
    "            s.append(x[sb:se])\n",
    "    for sec in s:\n",
    "        x = x.replace(sec, \"\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    \"\"\"\n",
    "    Function to clean the text\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED:\n",
    "        0: No Cleaning\n",
    "        1: Light Cleaning\n",
    "        4: Heavy Cleaning\n",
    "    \"\"\"\n",
    "    if CLEANING_LEVEL == 0:\n",
    "        return x\n",
    "    elif CLEANING_LEVEL == 1:\n",
    "        return clean_headings(x)\n",
    "    elif CLEANING_LEVEL == 2:\n",
    "        return clean_headings(x, DEL_HEADINGS=True)\n",
    "    elif CLEANING_LEVEL == 3:\n",
    "        return remove_sections(x)\n",
    "    elif CLEANING_LEVEL == 4:\n",
    "        return clean_headings(remove_sections(x))\n",
    "    elif CLEANING_LEVEL == 5:\n",
    "        return clean_headings(remove_sections(x), DEL_HEADINGS=True)\n",
    "    else:\n",
    "        raise Exception(\"Invalid CLEANING_LEVEL configured. Please check the config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.10.1+cu113 Device: cuda [NVIDIA GeForce GTX 1650]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    devicename = \"[\" + torch.cuda.get_device_name(0) + \"]\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    devicename = \"\"\n",
    "\n",
    "print(\"Using PyTorch version:\", torch.__version__, \"Device:\", device, devicename)\n",
    "assert Version(torch.__version__) >= Version(\"1.0.0\"), \"Please install PyTorch version >= 1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "{'B': 0}\n",
      "{'B': 0, 'C': 1}\n",
      "{'B': 0, 'C': 1, 'FA': 2}\n",
      "{'B': 0, 'C': 1, 'FA': 2, 'GA': 3}\n",
      "{'B': 0, 'C': 1, 'FA': 2, 'GA': 3, 'Start': 4}\n",
      "{'B': 0, 'C': 1, 'FA': 2, 'GA': 3, 'Start': 4, 'Stub': 5}\n",
      "Found 3244 texts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"DATADIR\" in os.environ:\n",
    "    DATADIR = os.environ[\"DATADIR\"]\n",
    "else:\n",
    "    DATADIR = os.getcwd()\n",
    "\n",
    "TEXT_DATA_DIR = os.path.join(DATADIR, \"TEST\")\n",
    "\n",
    "print(\"Processing text dataset\")\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        print(labels_index)\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {\"encoding\": \"latin-1\"}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    t = clean(t)\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print(\"Found %s texts.\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3244\n",
      "{'B': 0, 'C': 1, 'FA': 2, 'GA': 3, 'Start': 4, 'Stub': 5}\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training texts: 2920\n",
      "Length of training labels: 2920\n",
      "Length of test texts: 324\n",
      "Length of test labels: 324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEST_SET = int(len(texts)*0.10)   # 10% of the data for testing\n",
    "\n",
    "(sentences_train, sentences_test,\n",
    " labels_train, labels_test) = train_test_split(texts, labels, test_size=TEST_SET, shuffle=True, random_state=42)\n",
    "\n",
    "print('Length of training texts:', len(sentences_train))\n",
    "print('Length of training labels:', len(labels_train))\n",
    "print('Length of test texts:', len(sentences_test))\n",
    "print('Length of test labels:', len(labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first training sentence:\n",
      "[CLS] Patrizia Spuri (born 18 February 1973 in Fara in Sabina) is an Italian former sprinter (400 m) and middle distance runner (800 m).\n",
      "In her career she won 9 times the national championships. She's the wife of the triple jumper Fabrizio Donato.\n",
      "\n",
      "\n",
      "National records\n",
      "4x400 metres relay: 3'26\"69 ( Paris, 20 June 1999) - with Virna De Angeli, Francesca Carbone, Danielle Perpoli\n",
      "4x400 metres relay indoor: 3'35\"01 ( Ghent, 27 February 2000) - with Virna De Angeli, Francesca Carbone, Carla Barbarino\n",
      "\n",
      "\n",
      "Achievements\n",
      "\n",
      "\n",
      "National titles\n",
      "4 wins in 400 metres at the Italian Athletics Championships (1994, 1996, 1997, 1998)\n",
      "1 win in 800 metres at the Italian Athletics Championships (1999)\n",
      "2 wins in 400 metres at the Italian Athletics Indoor Championships (1994, 1998)\n",
      "1 win in 800 metres at the Italian Athletics Indoor Championships (2000)\n",
      "\n",
      "\n",
      "See also\n",
      "Italian all-time top lists - 400 metres\n",
      "Italian all-time top lists - 800 metres\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "External links\n",
      "Patrizia Spuri at World Athletics\n",
      "Patrizia Spuri at European Athletic Association (archived)\n",
      "Patrizia Spuri at Italian Athletics Federation (in Italian)\n",
      "Patrizia Spuri at Olympedia\n",
      "Patrizia Spuri at Olympics at Sports-Reference.com (archived)\n",
      "Patrizia Spuri at The-Sports.org LABEL: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "\n",
    "print (\"The first training sentence:\")\n",
    "print(sentences_train[0], 'LABEL:', labels_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BertTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 878k/878k [00:01<00:00, 778kB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:01<00:00, 400kB/s]  \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:01<00:00, 815kB/s] \n",
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 240kB/s]\n"
     ]
    }
   ],
   "source": [
    "print('Initializing RoBERTaTokenizer')\n",
    "\n",
    "ROBERTA_MODEL = 'roberta-base'\n",
    "CACHE_DIR=os.path.join(DATADIR, 'transformers-cache')\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", cache_dir=CACHE_DIR, do_lower_case=True,max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6894 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full tokenized first training sentence:\n",
      "['[', 'CL', 'S', ']', 'ĠPatri', 'z', 'ia', 'ĠSp', 'uri', 'Ġ(', 'born', 'Ġ18', 'ĠFebruary', 'Ġ1973', 'Ġin', 'ĠF', 'ara', 'Ġin', 'ĠSab', 'ina', ')', 'Ġis', 'Ġan', 'ĠItalian', 'Ġformer', 'Ġspr', 'inter', 'Ġ(', '400', 'Ġm', ')', 'Ġand', 'Ġmiddle', 'Ġdistance', 'Ġrunner', 'Ġ(', '800', 'Ġm', ').', 'Ċ', 'In', 'Ġher', 'Ġcareer', 'Ġshe', 'Ġwon', 'Ġ9', 'Ġtimes', 'Ġthe', 'Ġnational', 'Ġchampionships', '.', 'ĠShe', \"'s\", 'Ġthe', 'Ġwife', 'Ġof', 'Ġthe', 'Ġtriple', 'Ġjumper', 'ĠFab', 'riz', 'io', 'ĠDon', 'ato', '.', 'ĊĊ', 'Ċ', 'National', 'Ġrecords', 'Ċ', '4', 'x', '400', 'Ġmetres', 'Ġrelay', ':', 'Ġ3', \"'\", '26', '\"', '69', 'Ġ(', 'ĠParis', ',', 'Ġ20', 'ĠJune', 'Ġ1999', ')', 'Ġ-', 'Ġwith', 'ĠVir', 'na', 'ĠDe', 'ĠAngel', 'i', ',', 'ĠFrances', 'ca', 'ĠCar', 'bone', ',', 'ĠDanielle', 'ĠPer', 'p', 'oli', 'Ċ', '4', 'x', '400', 'Ġmetres', 'Ġrelay', 'Ġindoor', ':', 'Ġ3', \"'\", '35', '\"', '01', 'Ġ(', 'ĠG', 'hent', ',', 'Ġ27', 'ĠFebruary', 'Ġ2000', ')', 'Ġ-', 'Ġwith', 'ĠVir', 'na', 'ĠDe', 'ĠAngel', 'i', ',', 'ĠFrances', 'ca', 'ĠCar', 'bone', ',', 'ĠCar', 'la', 'ĠBarbar', 'ino', 'ĊĊ', 'Ċ', 'A', 'chieve', 'ments', 'ĊĊ', 'Ċ', 'National', 'Ġtitles', 'Ċ', '4', 'Ġwins', 'Ġin', 'Ġ400', 'Ġmetres', 'Ġat', 'Ġthe', 'ĠItalian', 'ĠAthletics', 'ĠChampionships', 'Ġ(', '1994', ',', 'Ġ1996', ',', 'Ġ1997', ',', 'Ġ1998', ')', 'Ċ', '1', 'Ġwin', 'Ġin', 'Ġ800', 'Ġmetres', 'Ġat', 'Ġthe', 'ĠItalian', 'ĠAthletics', 'ĠChampionships', 'Ġ(', '1999', ')', 'Ċ', '2', 'Ġwins', 'Ġin', 'Ġ400', 'Ġmetres', 'Ġat', 'Ġthe', 'ĠItalian', 'ĠAthletics', 'ĠInd', 'oor', 'ĠChampionships', 'Ġ(', '1994', ',', 'Ġ1998', ')', 'Ċ', '1', 'Ġwin', 'Ġin', 'Ġ800', 'Ġmetres', 'Ġat', 'Ġthe', 'ĠItalian', 'ĠAthletics', 'ĠInd', 'oor', 'ĠChampionships', 'Ġ(', '2000', ')', 'ĊĊ', 'Ċ', 'See', 'Ġalso', 'Ċ', 'Italian', 'Ġall', '-', 'time', 'Ġtop', 'Ġlists', 'Ġ-', 'Ġ400', 'Ġmetres', 'Ċ', 'Italian', 'Ġall', '-', 'time', 'Ġtop', 'Ġlists', 'Ġ-', 'Ġ800', 'Ġmetres', 'ĊĊ', 'Ċ', 'References', 'ĊĊ', 'Ċ', 'External', 'Ġlinks', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠWorld', 'ĠAthletics', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠEuropean', 'ĠAthletic', 'ĠAssociation', 'Ġ(', 'arch', 'ived', ')', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠItalian', 'ĠAthletics', 'ĠFederation', 'Ġ(', 'in', 'ĠItalian', ')', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠOlymp', 'edia', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠOlympics', 'Ġat', 'ĠSports', '-', 'Reference', '.', 'com', 'Ġ(', 'arch', 'ived', ')', 'Ċ', 'Pat', 'riz', 'ia', 'ĠSp', 'uri', 'Ġat', 'ĠThe', '-', 'Sports', '.', 'org']\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The truncated tokenized first training sentence:\n",
      "['[', 'CL', 'S', ']', 'ĠPatri', 'z', 'ia', 'ĠSp', 'uri', 'Ġ(', 'born', 'Ġ18', 'ĠFebruary', 'Ġ1973', 'Ġin', 'ĠF', 'ara', 'Ġin', 'ĠSab', 'ina', ')', 'Ġis', 'Ġan', 'ĠItalian', 'Ġformer', 'Ġspr', 'inter', 'Ġ(', '400', 'Ġm', ')', 'Ġand', 'Ġmiddle', 'Ġdistance', 'Ġrunner', 'Ġ(', '800', 'Ġm', ').', 'Ċ', 'In', 'Ġher', 'Ġcareer', 'Ġshe', 'Ġwon', 'Ġ9', 'Ġtimes', 'Ġthe', 'Ġnational', 'Ġchampionships', '.', 'ĠShe', \"'s\", 'Ġthe', 'Ġwife', 'Ġof', 'Ġthe', 'Ġtriple', 'Ġjumper', 'ĠFab', 'riz', 'io', 'ĠDon', 'ato', '.', 'ĊĊ', 'Ċ', 'National', 'Ġrecords', 'Ċ', '4', 'x', '400', 'Ġmetres', 'Ġrelay', ':', 'Ġ3', \"'\", '26', '\"', '69', 'Ġ(', 'ĠParis', ',', 'Ġ20', 'ĠJune', 'Ġ1999', ')', 'Ġ-', 'Ġwith', 'ĠVir', 'na', 'ĠDe', 'ĠAngel', 'i', ',', 'ĠFrances', 'ca', 'ĠCar', 'bone', ',', 'ĠDanielle', 'ĠPer', 'p', 'oli', 'Ċ', '4', 'x', '400', 'Ġmetres', 'Ġrelay', 'Ġindoor', ':', 'Ġ3', \"'\", '35', '\"', '01', 'Ġ(', 'ĠG', 'hent', ',', 'Ġ27', 'ĠFebruary', 'Ġ2000', ')', 'Ġ-', 'SEP']\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices of the first training sentence:\n",
      "[10975  7454   104   742 24835   329   493  2064  6151    36  5400   504\n",
      "   902 14757    11   274  1742    11  6371  1243    43    16    41  3108\n",
      "   320 11085  8007    36  4017   475    43     8  1692  4472  7449    36\n",
      "  3913   475   322 50118  1121    69   756    79   351   361   498     5\n",
      "   632  8226     4   264    18     5  1141     9     5  6436 16338  8659\n",
      " 21645  1020  1599  3938     4 50140 50118 18285  2189 50118   306  1178\n",
      "  4017  7472 12937    35   155   108  2481   113  4563    36  2201     6\n",
      "   291   502  6193    43   111    19  9541  2133   926  6896   118     6\n",
      " 11442  3245  1653 18026     6 15156  2595   642  6483 50118   306  1178\n",
      "  4017  7472 12937 11894    35   155   108  2022   113  2663    36   272\n",
      " 37754     6   974   902  3788    43   111     3]\n"
     ]
    }
   ],
   "source": [
    "# Next we use the roBERTa tokenizer to convert each token into an integer\n",
    "# index in the roBERTa vocabulary. We also pad any shorter sequences to\n",
    "# `MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros.\n",
    "\n",
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT also requires *attention masks*, with 1 for each real token in\n",
    "# the sequences and 0 for the padding:\n",
    "\n",
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "\n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use again scikit-learn's train_test_split to use 10% of our\n",
    "# training data as a validation set, and then convert all data into\n",
    "# torch.tensors.\n",
    "\n",
    "(train_inputs, validation_inputs,train_labels, validation_labels) = train_test_split(ids_train, labels_train, random_state=42, test_size=0.1)\n",
    "(train_masks, validation_masks, _, _) = train_test_split(amasks_train, ids_train, random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(ids_test)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "test_masks  = torch.tensor(amasks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2628 messages\n",
      "Validation: 292 messages\n",
      "Test: 324 messages\n"
     ]
    }
   ],
   "source": [
    "# Next we create PyTorch DataLoaders for all data sets.\n",
    "#\n",
    "# For fine-tuning roBERTa on a specific task, the authors recommend a\n",
    "# batch size of 16 or 32.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'messages')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'messages')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/\n",
    "\n",
    "https://jesusleal.io/2020/10/20/RoBERTA-Text-Classification/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a26d0d8eb587ead6a281759cb4c3f7a35382d5f9d22086b6f7e3324913b9180"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
