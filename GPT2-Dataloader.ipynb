{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = 'TEST/'\n",
    "INDEX_PATH = os.path.join(BASE_FOLDER,\"index.tsv\")\n",
    "CLEAN_LEVEL = 4\n",
    "CACHE_DIR = os.path.join(os.getcwd(), 'transformers-cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tsv_file,\n",
    "        root_dir,\n",
    "        clean_level=0,\n",
    "        delete_sections=[\n",
    "            \"References\",\n",
    "            \"External links\",\n",
    "            \"See also\",\n",
    "            \"Further reading\",\n",
    "            \"Notes\",\n",
    "            \"Bibliography\",\n",
    "            \"Sources\",\n",
    "        ],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tsv_file (string): Path to the tsv file with annotations.\n",
    "            root_dir (string): Database Base Directory.\n",
    "            clean_level (callable, optional): Optional cleaning to be applied on a sample.\n",
    "            delete_sections (list, optional): Optional list of sections to be deleted if clean level is configured to delete sections.\n",
    "        \"\"\"\n",
    "        self.wiki_frame = pd.read_csv(tsv_file, sep=\"\\t\")\n",
    "        self.root_dir = root_dir\n",
    "        self.clean_level = clean_level\n",
    "        self.RE_HEADINGS = re.compile(r\"==.*?==+\", re.MULTILINE)\n",
    "        self.delete_sections = list(map(self._headingClean, delete_sections))\n",
    "\n",
    "    def _headingClean(self,x,args=None):\n",
    "        \"\"\"\n",
    "            internal function to clean headings and make them lowercase so that comparisons can be performed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return x.replace(\" \", \"\").lower()\n",
    "        except Exception as e:\n",
    "            print(\"ERROR at clean_heading function:\", e)\n",
    "            return x\n",
    "\n",
    "    def cleanHeadings(self, x, DEL_HEADINGS=False):\n",
    "        \"\"\"\n",
    "            Function to remove unwanted characters from headings\n",
    "        \"\"\"\n",
    "        if DEL_HEADINGS:\n",
    "            return self.RE_HEADINGS.sub(\"\", x)\n",
    "        else:\n",
    "            return (\n",
    "                x.replace(\"==== \", \"\")\n",
    "                .replace(\"=== \", \"\")\n",
    "                .replace(\"== \", \"\")\n",
    "                .replace(\" ====\", \"\")\n",
    "                .replace(\" ===\", \"\")\n",
    "                .replace(\" ==\", \"\")\n",
    "            )\n",
    "\n",
    "    def removeSections(self, x):\n",
    "        \"\"\"\n",
    "            Function to remove unwanted sections from the text\n",
    "        \"\"\"\n",
    "        r = self.RE_HEADINGS.finditer(x)\n",
    "        sections = [(m.start(0), m.end(0)) for m in r]\n",
    "        s = []\n",
    "        for i, sec in enumerate(sections):\n",
    "            secname = x[sec[0] : sec[1]].replace(\"=\", \"\").replace(\" \", \"\").lower()\n",
    "            if secname in self.delete_sections:\n",
    "                sb = sec[0]\n",
    "                try:\n",
    "                    se = sections[i + 1][0]\n",
    "                except IndexError:\n",
    "                    se = len(x)\n",
    "                s.append(x[sb:se])\n",
    "        for sec in s:\n",
    "            x = x.replace(sec, \"\")\n",
    "        return x\n",
    "\n",
    "    def clean(self, x):\n",
    "        \"\"\"\n",
    "            Function to clean the text\n",
    "            CLEANING LEVELS:\n",
    "                0: No cleaning\n",
    "                1: Clean All Headings\n",
    "                2: Delete All Headings\n",
    "                3: Delete Sections only\n",
    "                4: Delete Selected Sections and Clean Headings\n",
    "                5: Delete Selected Sections and Delete All Headings\n",
    "            RECOMMENDED:\n",
    "                0: No Cleaning\n",
    "                1: Light Cleaning\n",
    "                4: Heavy Cleaning\n",
    "        \"\"\"\n",
    "        if self.clean_level == 0:\n",
    "            return x\n",
    "        elif self.clean_level == 1:\n",
    "            return self.cleanHeadings(x)\n",
    "        elif self.clean_level == 2:\n",
    "            return self.cleanHeadings(x, DEL_HEADINGS=True)\n",
    "        elif self.clean_level == 3:\n",
    "            return self.removeSections(x)\n",
    "        elif self.clean_level == 4:\n",
    "            return self.cleanHeadings(self.removeSections(x))\n",
    "        elif self.clean_level == 5:\n",
    "            return self.cleanHeadings(self.removeSections(x), DEL_HEADINGS=True)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid clean_level configured. Please reinitialize dataloader.\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wiki_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        x = self.wiki_frame.iloc[idx]\n",
    "        cat, file_name = x[\"category\"], x[\"filename\"]\n",
    "        txt = open(os.path.join(self.root_dir, cat, file_name), \"r\").read()\n",
    "        txt = self.clean(txt)\n",
    "        sample = {\"text\": txt, \"label\": cat}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikipediaDataset(INDEX_PATH, BASE_FOLDER, clean_level=CLEAN_LEVEL)\n",
    "length = len(dataset)\n",
    "print(f\"Dataset length: {length}\")\n",
    "test_l = int(length * 0.15)\n",
    "train_l = length - test_l\n",
    "valid_l = int(train_l * 0.1)\n",
    "train_l -= valid_l\n",
    "train,test,validation = random_split(range(len(dataset)), [train_l,test_l,valid_l], generator=torch.Generator().manual_seed(42))\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}, Validation: {len(validation)}\")\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'B': 0, 'C': 1, 'FA': 2, 'GA': 3, 'Start': 4, 'Stub': 5}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton task. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import set_seed, GPT2Tokenizer\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(42)\n",
    "\n",
    "# Number of training epochs.\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path, cache_dir=CACHE_DIR)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print('Dealing with Test...')\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(validation, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb#scrollTo=EDEubgJIt23C"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6bdd94c198576e93dd5c2f539d34e9906db66a8c6bc385acd1638e2dac1003d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
