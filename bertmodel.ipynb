{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# ## BERT model initialization\n",
    "#\n",
    "# We now load a pretrained BERT model with a single linear\n",
    "# classification layer added on top.\n",
    "\n",
    "print(\"Initializing BertForSequenceClassification\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERTMODEL, cache_dir=CACHE_DIR, num_labels=20\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "# We set the remaining hyperparameters needed for fine-tuning the\n",
    "# pretrained model:\n",
    "#   * EPOCHS: the number of training epochs in fine-tuning\n",
    "#     (recommended values between 2 and 4)\n",
    "#   * WEIGHT_DECAY: weight decay for the Adam optimizer\n",
    "#   * LR: learning rate for the Adam optimizer (2e-5 to 5e-5 recommended)\n",
    "#   * WARMUP_STEPS: number of warmup steps to (linearly) reach the set\n",
    "#     learning rate\n",
    "#\n",
    "# We also need to grab the training parameters from the pretrained model.\n",
    "\n",
    "EPOCHS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR = 2e-5\n",
    "WARMUP_STEPS = int(0.2 * len(train_dataloader))\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=len(train_dataloader) * EPOCHS,\n",
    ")\n",
    "\n",
    "\n",
    "# ## Learning\n",
    "#\n",
    "# Let's now define functions to train() and evaluate() the model:\n",
    "\n",
    "\n",
    "def train(epoch, loss_vector=None, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Loop over each batch from the training set\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Copy data to GPU if needed\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        if loss_vector is not None:\n",
    "            loss_vector.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    step * len(b_input_ids),\n",
    "                    len(train_dataloader.dataset),\n",
    "                    100.0 * step / len(train_dataloader),\n",
    "                    loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    n_correct, n_all = 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids, token_type_ids=None, attention_mask=b_input_mask\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "        labels = b_labels.to(\"cpu\").numpy()\n",
    "        n_correct += np.sum(predictions == labels)\n",
    "        n_all += len(labels)\n",
    "\n",
    "    print(\"Accuracy: [{}/{}] {:.4f}\\n\".format(n_correct, n_all, n_correct / n_all))\n",
    "\n",
    "\n",
    "# Now we are ready to train our model using the train()\n",
    "# function. After each epoch, we evaluate the model using the\n",
    "# validation set and evaluate().\n",
    "\n",
    "train_lossv = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, train_lossv)\n",
    "    print(\"\\nValidation set:\")\n",
    "    evaluate(validation_dataloader)\n",
    "\n",
    "\n",
    "# ## Inference\n",
    "#\n",
    "# For a better measure of the quality of the model, let's see the\n",
    "# model accuracy for the test messages.\n",
    "\n",
    "print(\"Test set:\")\n",
    "evaluate(test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
