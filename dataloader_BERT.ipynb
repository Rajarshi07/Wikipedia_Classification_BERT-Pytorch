{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "from packaging.version import Version\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io, sys, os, datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Functions and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLEANING CONFIG\n",
    "DELETE_SECTIONS = [\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"See also\",\n",
    "    \"Further reading\",\n",
    "    \"Notes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "]\n",
    "CLEANING_LEVEL = 1\n",
    "\"\"\"\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED: \n",
    "        0: No Cleaning\n",
    "        1: Light Cleaning\n",
    "        4: Heavy Cleaning\n",
    "\"\"\"\n",
    "# END CONFIG\n",
    "\n",
    "## UTILITY FUNCTIONS TO MAKE CONFIG USABLE\n",
    "\n",
    "\n",
    "def _heading_clean(x):\n",
    "    \"\"\"\n",
    "    internal function to clean headings and make them lowercase so that comparisons can be performed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return x.replace(\" \", \"\").lower()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR at clean_heading function:\", e)\n",
    "        return x\n",
    "\n",
    "\n",
    "DELETE_SECTIONS = list(map(_heading_clean, DELETE_SECTIONS))\n",
    "\n",
    "## END UTILITY FUNCTIONS\n",
    "\n",
    "## CLEANER FUNCTIONS\n",
    "RE_HEADINGS = re.compile(r\"==.*?==+\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def clean_headings(x, DEL_HEADINGS=False):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted characters from headings\n",
    "    Configurable to remove headings or not via DELETE_HEADINGS.\n",
    "    Configurable to clean headings or not via CLEAN_HEADINGS.\n",
    "    Warning: This function will remove all headings from the text. Please run only after deleting unwanted sections.\n",
    "    \"\"\"\n",
    "    if DEL_HEADINGS:\n",
    "        return RE_HEADINGS.sub(\"\", x)\n",
    "    else:\n",
    "        return (\n",
    "            x.replace(\"==== \", \"\")\n",
    "            .replace(\"=== \", \"\")\n",
    "            .replace(\"== \", \"\")\n",
    "            .replace(\" ====\", \"\")\n",
    "            .replace(\" ===\", \"\")\n",
    "            .replace(\" ==\", \"\")\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_sections(x):\n",
    "    \"\"\"\n",
    "    Function to remove unwanted sections from the text\n",
    "    Configurable via DELETE_SECTIONS\n",
    "    \"\"\"\n",
    "    r = RE_HEADINGS.finditer(x)\n",
    "    sections = [(m.start(0), m.end(0)) for m in r]\n",
    "    s = []\n",
    "    for i, sec in enumerate(sections):\n",
    "        secname = x[sec[0] : sec[1]].replace(\"=\", \"\").replace(\" \", \"\").lower()\n",
    "        if secname in DELETE_SECTIONS:\n",
    "            sb = sec[0]\n",
    "            try:\n",
    "                se = sections[i + 1][0]\n",
    "            except IndexError:\n",
    "                se = len(x)\n",
    "            s.append(x[sb:se])\n",
    "    for sec in s:\n",
    "        x = x.replace(sec, \"\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    \"\"\"\n",
    "    Function to clean the text\n",
    "    CLEANING LEVELS:\n",
    "        0: No cleaning\n",
    "        1: Clean All Headings\n",
    "        2: Delete All Headings\n",
    "        3: Delete Sections only\n",
    "        4: Delete Selected Sections and Clean Headings\n",
    "        5: Delete Selected Sections and Delete All Headings\n",
    "    RECOMMENDED:\n",
    "        0: No Cleaning\n",
    "        1: Light Cleaning\n",
    "        4: Heavy Cleaning\n",
    "    \"\"\"\n",
    "    if CLEANING_LEVEL == 0:\n",
    "        return x\n",
    "    elif CLEANING_LEVEL == 1:\n",
    "        return clean_headings(x)\n",
    "    elif CLEANING_LEVEL == 2:\n",
    "        return clean_headings(x, DEL_HEADINGS=True)\n",
    "    elif CLEANING_LEVEL == 3:\n",
    "        return remove_sections(x)\n",
    "    elif CLEANING_LEVEL == 4:\n",
    "        return clean_headings(remove_sections(x))\n",
    "    elif CLEANING_LEVEL == 5:\n",
    "        return clean_headings(remove_sections(x), DEL_HEADINGS=True)\n",
    "    else:\n",
    "        raise Exception(\"Invalid CLEANING_LEVEL configured. Please check the config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    devicename = \"[\" + torch.cuda.get_device_name(0) + \"]\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    devicename = \"\"\n",
    "\n",
    "print(\"Using PyTorch version:\", torch.__version__, \"Device:\", device, devicename)\n",
    "assert Version(torch.__version__) >= Version(\"1.0.0\"), \"Please install PyTorch version >= 1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"DATADIR\" in os.environ:\n",
    "    DATADIR = os.environ[\"DATADIR\"]\n",
    "else:\n",
    "    DATADIR = os.getcwd()\n",
    "\n",
    "TEXT_DATA_DIR = os.path.join(DATADIR, \"TEST\")\n",
    "\n",
    "print(\"Processing text dataset\")\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        print(labels_index)\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {\"encoding\": \"latin-1\"}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    t = clean(t)\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print(\"Found %s texts.\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST_SET = int(len(texts)*0.10)   # 10% of the data for testing\n",
    "\n",
    "(sentences_train, sentences_test,\n",
    " labels_train, labels_test) = train_test_split(texts, labels, test_size=TEST_SET, shuffle=True, random_state=42)\n",
    "\n",
    "print('Length of training texts:', len(sentences_train))\n",
    "print('Length of training labels:', len(labels_train))\n",
    "print('Length of test texts:', len(sentences_test))\n",
    "print('Length of test labels:', len(labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "# sentences_validation = [\"[CLS] \" + s for s in sentences_validation]\n",
    "print (\"The first training sentence:\")\n",
    "print(sentences_train[0], 'LABEL:', labels_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing BertTokenizer')\n",
    "\n",
    "BERTMODEL='bert-base-uncased'\n",
    "CACHE_DIR=os.path.join(DATADIR, 'transformers-cache')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL, cache_dir=CACHE_DIR, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we use the BERT tokenizer to convert each token into an integer\n",
    "# index in the BERT vocabulary. We also pad any shorter sequences to\n",
    "# `MAX_LEN_TRAIN` or `MAX_LEN_TEST` indices with trailing zeros.\n",
    "\n",
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT also requires *attention masks*, with 1 for each real token in\n",
    "# the sequences and 0 for the padding:\n",
    "\n",
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "\n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use again scikit-learn's train_test_split to use 10% of our\n",
    "# training data as a validation set, and then convert all data into\n",
    "# torch.tensors.\n",
    "\n",
    "(train_inputs, validation_inputs,train_labels, validation_labels) = train_test_split(ids_train, labels_train, random_state=42, test_size=0.1)\n",
    "(train_masks, validation_masks, _, _) = train_test_split(amasks_train, ids_train, random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(ids_test)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "test_masks  = torch.tensor(amasks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create PyTorch DataLoaders for all data sets.\n",
    "#\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a\n",
    "# batch size of 16 or 32.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'messages')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'messages')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/\n",
    "\n",
    "https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "\n",
    "https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a26d0d8eb587ead6a281759cb4c3f7a35382d5f9d22086b6f7e3324913b9180"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
